# DATAFLOW Pipeline Audit, Stage Visibility, and Staleness

This document captures the current audit system and the key lists/queues that control whether a datafile (basename) is brought, processed, or skipped. The goal is to provide a quick, reliable view of where each basename sits in the pipeline, and to avoid reprocessing unless explicitly requested.

## Quick View (Most Useful Commands)

### 1) Run the audit (summary-only or full)

```bash
# Summary-only (fast): counts per stage/flag
python3 MASTER/ANCILLARY/PIPELINE_OPERATIONS/AUDIT_PIPELINE_STATES/audit_pipeline_states.py --stations 0 --summary-only

# Full (includes per-basename matrix)
python3 MASTER/ANCILLARY/PIPELINE_OPERATIONS/AUDIT_PIPELINE_STATES/audit_pipeline_states.py --stations 0

# Full + stale detection threshold (hours)
python3 MASTER/ANCILLARY/PIPELINE_OPERATIONS/AUDIT_PIPELINE_STATES/audit_pipeline_states.py --stations 0 --stale-hours 24

# Full + scan logs for basename mentions (slower)
python3 MASTER/ANCILLARY/PIPELINE_OPERATIONS/AUDIT_PIPELINE_STATES/audit_pipeline_states.py --stations 0 --scan-logs

# Cap HTML rows (default 3000)
python3 MASTER/ANCILLARY/PIPELINE_OPERATIONS/AUDIT_PIPELINE_STATES/audit_pipeline_states.py --stations 0 --html-max-rows 3000
```

Outputs go to:
```
MASTER/ANCILLARY/PIPELINE_OPERATIONS/AUDIT_PIPELINE_STATES/OUTPUT_FILES/<timestamp>/
```

### 2) Inspect the audit outputs

- `inventory_paths.csv` – every list/queue path + counts
- `summary_counts.csv` – counts for each state flag
- `anomalies.csv` – basenames in suspicious states
- `basename_state.csv` – full per-basename matrix (only if not `--summary-only`)
- `stale.csv` – basenames with no activity for the stale threshold
- `report.html` – visual report (summary + anomalies + stale + top basenames)

### 3) Use the status GUI for recent activity

```bash
python3 MASTER/ANCILLARY/PIPELINE_REAL_TIME_CHECK/step1_status_timeline_gui.py --stations 0 --tasks 1,2,3,4,5
```

## Canonical Lists That Control “Already Seen” / “Reject” Behavior

These are the lists that the pipeline reads to skip work or avoid duplicates.

### Stage 0 / New Files

- `STATIONS/MINGO0{station}/STAGE_0/NEW_FILES/METADATA/raw_files_brought.csv`
  - Used by: `MASTER/STAGE_0/NEW_FILES/bring_data_and_config_files.sh`
  - Meaning: raw `.dat` already pulled from station (skip on future brings).

### Stage 0 / Simulation (station 0 only)

- `STATIONS/MINGO00/STAGE_0/imported_basenames.csv`
  - Used by: `MASTER/STAGE_0/SIMULATION/ingest_simulated_station_data.py`
  - Meaning: simulated `.dat` basenames already imported.

### Stage 0 / Reprocessing

- `STATIONS/MINGO0{station}/STAGE_0/REPROCESSING/STEP_1/METADATA/hld_files_brought.csv`
  - Used by: `MASTER/STAGE_0/REPROCESSING/STEP_1/bring_reprocessing_files.sh`
  - Meaning: HLD basenames already brought (skip future downloads).

- `STATIONS/MINGO0{station}/STAGE_0/REPROCESSING/STEP_2/METADATA/dat_files_unpacked.csv`
  - Used by: `MASTER/STAGE_0/REPROCESSING/STEP_2/unpack_reprocessing_files.sh`
  - Meaning: `.dat` already unpacked (skip unless override).

### Processed Basenames (Final “Do Not Reprocess” List)

- `MASTER/ANCILLARY/PIPELINE_OPERATIONS/UPDATE_EXECUTION_CSVS/OUTPUT_FILES/MINGO0{station}_processed_basenames.csv`
  - Generated by: `MASTER/ANCILLARY/PIPELINE_OPERATIONS/UPDATE_EXECUTION_CSVS/update_execution_csvs.sh`
  - Source: STEP_3 / TASK_2 outputs (`event_data_YYYY_MM_DD.csv`).
  - Used by:
    - `MASTER/STAGE_0/REPROCESSING/STEP_1/bring_reprocessing_files.sh`
    - `MASTER/ANCILLARY/PIPELINE_OPERATIONS/FLUSH_FROM_UNPROCESSED/flush_from_unprocessed.sh`
  - Meaning: basename has already made it through the pipeline and should not be re-brought or reprocessed unless explicitly requested.

### Config Flags That Control Skip Behavior

- `MASTER/CONFIG_FILES/config_global.yaml`
  - `use_processed_as_reject_list_{station}: true|false`
    - If `false`, processed basenames are excluded from reprocessing.
    - If `true`, processed basenames are NOT excluded (use with care).

## Pipeline Stage Map (Where Files Physically Are)

### Stage 0 → Stage 1 input

- `STATIONS/MINGO0{station}/STAGE_0_to_1` – raw `.dat` queue for STEP_1/TASK_1

### STEP 1 (Tasks 1–5)

Each task uses queues under:
```
STATIONS/MINGO0{station}/STAGE_1/EVENT_DATA/STEP_1/TASK_{n}/INPUT_FILES/
  UNPROCESSED_DIRECTORY
  PROCESSING_DIRECTORY
  COMPLETED_DIRECTORY
  ERROR_DIRECTORY
```

Outputs:
- Task 1 → `cleaned_*.parquet`
- Task 2 → `calibrated_*.parquet`
- Task 3 → `listed_*.parquet`
- Task 4 → `fitted_*.parquet`
- Task 5 → `corrected_*.parquet`

Final STEP_1 outputs are stored in:
- `STATIONS/MINGO0{station}/STAGE_1/EVENT_DATA/STEP_1_TO_2_OUTPUT`

### STEP 2

Queues:
```
STATIONS/MINGO0{station}/STAGE_1/EVENT_DATA/STEP_2/INPUT_FILES/
  UNPROCESSED
  PROCESSING
  COMPLETED
  ERROR_DIRECTORY
  REJECTED
```

Outputs:
- `STATIONS/MINGO0{station}/STAGE_1/EVENT_DATA/STEP_2_TO_3_OUTPUT/accumulated_*.csv`

### STEP 3

- Split daily outputs:
  - `STATIONS/MINGO0{station}/STAGE_1/EVENT_DATA/STEP_3/TASK_1_TO_2/YYYY/MM/DD/accumulated_*.csv`
- Joined daily outputs:
  - `STATIONS/MINGO0{station}/STAGE_1/EVENT_DATA/STEP_3/TASK_2/OUTPUT_FILES/YYYY/MM/event_data_YYYY_MM_DD.csv`

## Audit Script (What It Does)

The audit script normalizes basenames across:
- list/metadata CSVs (raw brought, HLD brought, dat unpacked, processed)
- queues (UNPROCESSED / PROCESSING / COMPLETED / ERROR)
- outputs (cleaned, calibrated, listed, fitted, corrected, accumulated)

It outputs:
- `summary_counts.csv` (counts per state flag)
- `basename_state.csv` (per-basename state matrix)
- `anomalies.csv` (suspect basenames)
- `stale.csv` (basenames older than stale threshold)
- `report.html` (visual report)

### Current anomaly checks

- `raw_brought_missing`: in raw_files_brought.csv but not found anywhere else
- `processed_but_queued`: in processed list but still in queues
- `reproc_hld_not_unpacked`: HLD brought but no dat unpacked
- `dat_unpacked_not_seen`: unpacked but no downstream outputs
- `stale`: last activity older than stale threshold (default 24 hours)

If `--scan-logs` is used, `anomalies.csv` and `stale.csv` also include `log_hint` with the most recent matching log line.

## “Do Not Reprocess Unless Explicitly Requested” Logic

The intended safe path is:

1. **If basename is in `*_processed_basenames.csv`:**
   - Do not reprocess unless explicitly forced.

2. **If basename is in reprocessing lists (`hld_files_brought.csv` or `dat_files_unpacked.csv`):**
   - Do not re-download/unpack unless explicitly forced.

3. **If basename is still queued (UNPROCESSED/PROCESSING):**
   - Let the pipeline finish before restarting or reprocessing.

4. **Only reprocess when explicitly requested** by:
   - Removing the basename from the relevant list(s), or
   - Using a pipeline flag/config that allows it (e.g., `use_processed_as_reject_list_{station}: true`).

## Staleness Strategy (What To Do When a Basename Stops Progressing)

### A) Detect staleness

A basename is “stale” when it appears in an early stage but not in any downstream stages after some threshold time. The audit output helps catch this quickly.

Recommended checks:
- If in raw_files_brought but not in any Task 1 queue/output or STEP_1_TO_2_OUTPUT.
- If in Task 1/2/3/4 queues but never reaches Task 5 output.
- If in STEP_1_TO_2_OUTPUT but not in STEP_2 queues or STEP_2_TO_3_OUTPUT.
- If in STEP_2_TO_3_OUTPUT but not in STEP_3 outputs.

### B) Identify “why” (common causes)

- File in ERROR_DIRECTORY (task or step error).
- Resource gate skipped processing (check logs under `EXECUTION_LOGS/CRON_LOGS`).
- Reject list blocked reprocessing.
- Corrupt or zero-length input.

### C) Reprocess paths (from light to heavy)

1. **Retry from the current step**
   - Move file from ERROR to UNPROCESSED in that step.

2. **Re-run a prior step**
   - Remove the basename from the step’s queue/output and re-run that step.

3. **Full reset** (if necessary)
   - Remove basename from `*_processed_basenames.csv` (only if you truly want to reprocess).
   - Remove from `hld_files_brought.csv` and `dat_files_unpacked.csv` if reprocessing Stage 0.
   - Move or delete downstream outputs to prevent inconsistent state.

## Notes / Limitations

- `ANCILLARY/REJECTED_FILES` folders contain per-run reject logs, not guaranteed to be per-basename lists. They are useful for manual investigation, not for stable basenames state tracking.
- The audit script normalizes basenames across suffixes and prefixes (`cleaned_`, `calibrated_`, etc.) to align states across the pipeline.

## Roadmap Ideas (Optional)

- Add “stale age” detection (using file mtimes and metadata timestamps).
- Extend `anomalies.csv` with failure reasons from error logs.
- Create a single consolidated UI (CSV or dashboard) that shows basenames + stage + age.
