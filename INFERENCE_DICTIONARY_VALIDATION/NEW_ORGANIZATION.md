I need a change of organization in /home/mingo/DATAFLOW_v3/INFERENCE_DICTIONARY_VALIDATION. The purpose of this section of the code is to collect simulated data results, create a good dictionary and tests its validity and limits. Hence, some changes must be implemented in the pipeline.

Step 1. Setting up

Step 1. Task 1. **Data collection and matching with simulation parameters**. First you are going to collect ALL the simulated data results, which are the ones you are taking now in /home/mingo/DATAFLOW_v3/STATIONS/MINGO00/STAGE_1/EVENT_DATA/STEP_1/TASK_*/METADATA/task_*_metadata_specific.csv, though now we are only doing it for task 1 (put in a /home/mingo/DATAFLOW_v3/INFERENCE_DICTIONARY_VALIDATION/to_do.md that this must be done) and relating each row with a row of the /home/mingo/DATAFLOW_v3/MINGO_DIGITAL_TWIN/SIMULATED_DATA/step_final_simulation_params.csv; this serves the sole purpose of identifying each dataset with a set of simulated parameters: this should be step 1.1, collect data. If a dataset from the simulation is not in the task_*_metadata_specific.csv, then do not even take the row, simply drop it, also drop the file_name column (not relevant anymore). In a config file it's necesary to indicate which z position configuration is going to be taken: we cannot be comparing everything, all the time, so now a cut is going to be done on the z positions. If no z position configuration is given, a random one amongst the possible ones will be selected.

Step 1. Task 2. **Dictionary and dataset creation**. The next step would be to filter some outliers in this large table. using some predefined checks, like the eff 2 and 3, remove those rows that are simply outliers. In this same step 1.2 the large table is going to be set as "data", and a copy of a subsample of this large table is going to be set as "dictionary". Note that the "data" contains entries that are directly in the dictionary, so watch out when comparing so that entry in the dictionary is removed for a comparison with the same row when it's data. Currently the pipeline manages this pretty good, but i want more clear the difference between the data sample and the dictionary sample. So the dictionary is chosen according to a certain goodness criteria, in this case the relative error of eff 2 and 3 being smaller than a certain % and the sample being larger than a certain number, also note that there should be only one entry per set of parameters (no entries with same parameters and different number of events should be in the dictionary, but the one with the largest number of counts). This creates indeed the dictionary. Here you include some plots of the dictionary coverage of the plane, the histograms and scatters in diagonals of the set of parameters that are data and are dictionary, etc. Keep it simple, actually, I don't need a lot of plots to see if there are biases and the dictionary looks nice.

Step 2. Inference study.

Step 2. Task 1. **Solution to the inverse problem**. Once the dictionary and the datasets are designed, the mission is to use it to reconstruct the simulated set of parameters. To do so, some columns are selected as components to build a vector and then some different "distances" are defined between the two vectors. The l2 with z scores is pretty good, though it would be interesting to attempt other distances to see the best, as well as other columns of interest, so leave that chance open. So, you are going to use an interpolation method for every point in the dataset to match against the dictionary so the function takes the dictionary path, the dataset path and returns estimated points. Make this function simple and isolated so later i can simply take it for any dictionary and any dataset, even though its not simulated. Going back to the proper script of step 2 task 1, given the results of the interpolation/estimation, do some plots. Note that this method would work for any number of parameters and any combination of them. Make the function of estimation the most self-contained possible, and call it from the main step 2 task 1 script.

Step 2. Task 2. **Validation of the solution**. Here you estimate the uncertainties and proceed with the validation of the method. Plot the estimated parameters vs the simulated parameters, the relative errors (better in the same plot), and also in the flux vs eff plot do a contour plot of the relative error in flux and other plot for the relative error in efficiency for all dataset points. Indicate the dictionary points with a cross or something like that. Do some plots to check how everything is working. Then, having the relative error is each parameter for each parameter and each number of events in the file tested in the dataset, save that for the next step.

Step 2. Task 3. **Uncertainty assessment**. Now, given the info of the estimated set of params, the simulated set of params, the number of events in the file, you are going to do a binning/meshig in every component of the set of params and in the number of events in the file, and you are going to plot the histogram of relative errors per each case, try to identify and remove outliers (still plot them in another colour), calculate quantiles and from that, per each bin, an uncertainty will be assigned per each parameter center of the binning/mesh. You then use these tables as LUTs in which you are going to interpolate: so when a value has a certian set of parameters as estimation, then in the LUT you check, interpolating, which uncertainty it would correspond to it. Save the LUT. In the first line, commented in #, you can put some info on the dictionary that gave origin to it.